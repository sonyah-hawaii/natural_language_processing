{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to the overall interaction (as measured by number of comments)?_\n",
    "\n",
    "Your method for acquiring the data will be scraping the 'hot' threads as listed on the [Reddit homepage](https://www.reddit.com/). You'll acquire _AT LEAST FOUR_ pieces of information about each thread:\n",
    "1. The title of the thread\n",
    "2. The subreddit that the thread corresponds to\n",
    "3. The length of time it has been up on Reddit\n",
    "4. The number of comments on the thread\n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts whether or not a given Reddit post will have above or below the _median_ number of comments.\n",
    "\n",
    "**BONUS PROBLEMS**\n",
    "1. If creating a logistic regression, GridSearch Ridge and Lasso for this model and report the best hyperparameter values.\n",
    "1. Scrape the actual text of the threads using Selenium (you'll learn about this in Webscraping II).\n",
    "2. Write the actual article that you're pitching and turn it into a blog post that you host on your personal website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target outcome: Identify characteristics of a post on Reddit that contribute the most to overall interaction (i.e. number of comments)\n",
    "\n",
    "### Attributes must include:\n",
    "- Title of thread\n",
    "- Subreddit\n",
    "- Length of time post has been up\n",
    "- Number of comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper\n",
    "\n",
    "https://praw.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictionary used as reference\n",
    "https://github.com/reddit-archive/reddit/wiki/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.reddit.com/hot.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'Bleep Bloop'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "res = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'kind']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_json['data'];\n",
    "#Everything we want is stored in 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after', 'before', 'children', 'dist', 'modhash']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data_json['data'].keys())\n",
    "#breaks down our information\n",
    "#after is the ID of the last post in this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_json['data']['children'][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_8m1gat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_json['data']['after'])\n",
    "#This is the id of the last comment in this post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "[post['data']['name'] for post in data_json['data']['children']];\n",
    "#names of all the blog posts on reddit\n",
    "#this is the anchor to use for the next time you hit reddit's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/hot.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Collect more information\n",
    "\n",
    "While we only require you to collect four features, there may be other info that you can find on the results page that might be useful. Feel free to write more functions so that you have more interesting and useful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'after': 't5_2s5ti'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url, params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = ''\n",
    "#t3_8mb92u'\n",
    "#t3_8m9kly'\n",
    "#t3_8mc2oz'\n",
    "#t3_8mc68o'\n",
    "#t3_8mdgts'\n",
    "#t3_8ma3gf'\n",
    "#t3_8matgl'\n",
    "#t3_8ma73u'\n",
    "#t3_8maqro'\n",
    "#t3_8mcgvx'\n",
    "#t3_8mcp0e'\n",
    "#t3_8mcq8j'\n",
    "#t3_8mcj7f'\n",
    "#t3_8mbhqm'\n",
    "#t3_8mbdn4'\n",
    "#t3_8md4ye'\n",
    "#t3_8mbjtc'\n",
    "#t3_8mcirf'\n",
    "#t3_8mdcnv'\n",
    "#t3_8mcvf3'\n",
    "#t3_8mc317'\n",
    "#t3_8mcqb5'\n",
    "#t5_2s5ti'\n",
    "#t3_8m1gat\n",
    "for i in range(300): #I increased my range and time.sleep with each additional link\n",
    "                     #I worked my way up, I wasn't sure how much my computer could take.\n",
    "    if after == None:\n",
    "        params = {}\n",
    "        print('Error. Check id number')\n",
    "    else:\n",
    "        params = {'after': after}\n",
    "    url = 'http://www.reddit.com/hot.json?{}'.format(after)\n",
    "    res = requests.get(url, params=params, headers = headers)\n",
    "    if res.status_code ==200:\n",
    "        data_json = res.json()\n",
    "        posts.extend(data_json['data']['children'])\n",
    "        after = data_json['data']['after']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break #stops the for loop if it receives an error code\n",
    "    time.sleep(3)\n",
    "print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[3857];\n",
    "#call the last post to get the name, input that back into the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I did this until I maxed out my requests and ended up with 8,000 unique posts. Decided that was enough and I moved on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31661"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts) #checking the number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8113"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([p['data']['name'] for p in posts])) #checking how many are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = set([p['data']['name'] for p in posts])\n",
    "#Saving this to an object so I can reference it later and pull only unique data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: build a for loop to pull individual bits of information and save it back to a `DataFrame`\n",
    "### A reminder of our goal: Identify characteristics of a post on Reddit that contribute the most to overall interaction (i.e. number of comments)Â¶\n",
    "### Attributes must include:\n",
    "- Title of thread\n",
    "- Subreddit\n",
    "- Length of time post has been up\n",
    "- Number of comments (yi)\n",
    "\n",
    "##### I'm going to go through a single post and pull information I feel could be useful, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p['data']['preview']['enabled']for p in posts[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527429342827.872\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ms = time.time()*1000.0\n",
    "print(ms)\n",
    "#Checking time for length of time up calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'preview'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-340-b574e1a6aa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mthread_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_video'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mthread_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saved'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saved'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mthread_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preview_avail'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preview'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enabled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'preview'"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for p in posts:\n",
    "    thread_info = {}\n",
    "    thread_info['comments'] = p['data']['num_comments']\n",
    "    thread_info['title'] = p['data']['title']\n",
    "    thread_info['subreddit'] = p['data']['subreddit']\n",
    "    thread_info['subreddit_subscribers'] = p['data']['subreddit_subscribers'] \n",
    "    thread_info['subreddit_type'] = p['data']['subreddit_type']\n",
    "    thread_info['ups'] = p['data']['ups']\n",
    "    thread_info['downs'] = p['data']['downs']\n",
    "    thread_info['score'] = p['data']['score']\n",
    "    thread_info['gilded'] = p['data']['gilded']\n",
    "    thread_info['can_gild'] = p['data']['can_gild']\n",
    "    thread_info['time_since_posted'] = (1527429342827.872-(p['data']['created'])) #time pulled above\n",
    "    thread_info['over_18'] = p['data']['over_18']\n",
    "    thread_info['distinguished'] = p['data']['distinguished']\n",
    "    thread_info['stickied'] = p['data']['stickied']\n",
    "    thread_info['archived'] = p['data']['archived']\n",
    "    thread_info['locked'] = p['data']['locked']\n",
    "    thread_info['num_crossposts'] = p['data']['num_crossposts']\n",
    "    thread_info['pinned'] = p['data']['pinned']\n",
    "    thread_info['post_categories'] = p['data']['post_categories']\n",
    "    thread_info['is_video'] = p['data']['is_video']\n",
    "    thread_info['saved'] = p['data']['saved']\n",
    "    #thread_info['preview_avail'] = p['data']['preview']['enabled']\n",
    "    \n",
    "    dataset.append(thread_info)\n",
    "pd.DataFrame(dataset).head()\n",
    "\n",
    "#Putting in multiple attributes to capture popularity\n",
    "#Will reduce this using a GridSearch to test penalties of model\n",
    "#Tried to select as many bools as possible to simplify when creating dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(dataset)['preview_avail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data dictionary that I fill as I add attributes to my dataset\n",
    "#Making notes on transformations to keep track of thoughts for data cleaning step\n",
    "dictionary = pd.DataFrame()\n",
    "dictionary['variable'] = ['comments',\n",
    "                          'title',\n",
    "                          'subreddit',\n",
    "                          'subreddit_subscribers',\n",
    "                          'subreddit_type',\n",
    "                          'ups',\n",
    "                          'downs',\n",
    "                          'score',\n",
    "                          'gilded',\n",
    "                          'can_gild',\n",
    "                          'time_since_posted',\n",
    "                          'over_18',\n",
    "                          'distinguished',\n",
    "                          'stickied',\n",
    "                          'archived',\n",
    "                          'locked',\n",
    "                          'num_crossposts',\n",
    "                          'pinned',\n",
    "                          'post_categories',\n",
    "                          'is_video',\n",
    "                          'saved',\n",
    "                          'preview_avail'\n",
    "                         ]\n",
    "\n",
    "dictionary['description'] = ['Number of comments. Target variable',\n",
    "                          'Title of post. Apply vectorizer',\n",
    "                          'subreddit name (Consider lemmatizer)',\n",
    "                          'Number of subreddit subscribers',\n",
    "                          'public v. private',\n",
    "                          'Number of uplikes',\n",
    "                          'Number of downlikes',\n",
    "                          'Net score(ups + downs)',\n",
    "                          'Number of times post is gilded',\n",
    "                          'Can post be gilded. gild = 0 if false',\n",
    "                          'Calculated as current_time - time_posted',\n",
    "                          'If post is 18+',\n",
    "                          'distinguished',\n",
    "                          'If post is stickied in post',\n",
    "                          'If post is archived',\n",
    "                          'If post is locked in post',\n",
    "                          'Times has been crossposted to other threads',\n",
    "                          'If post is pinned',\n",
    "                          'List of categories, Use LabelEncoder',\n",
    "                          'If post contains video media',\n",
    "                          'If post is saved',\n",
    "                          'If preview of post is visible'\n",
    "                         ]\n",
    "\n",
    "dictionary['dtype'] = ['str',\n",
    "                          'str',\n",
    "                          'subreddit',\n",
    "                          'int',\n",
    "                          'category',\n",
    "                          'int',\n",
    "                          'int',\n",
    "                          'int',\n",
    "                          'int',\n",
    "                          'bool',\n",
    "                          'long',\n",
    "                          'bool',\n",
    "                          'bool',\n",
    "                          'bool',\n",
    "                          'bool',\n",
    "                          'bool',\n",
    "                          'int',\n",
    "                          'bool',\n",
    "                          'list of str',\n",
    "                          'bool',\n",
    "                          'bool',\n",
    "                          'bool'\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_is(dataset, variable):\n",
    "    \"\"\"\n",
    "    Works on all standard Sonyah Seiden data dictionaries.\n",
    "    dataset = dataframe used to house data dictionary\n",
    "    variable = attribute information is needed on, formatted as string \"\"\"\n",
    "    return dataset[dataset['variable']==variable]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>description</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comments</td>\n",
       "      <td>Number of comments. Target variable</td>\n",
       "      <td>str</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable                          description dtype\n",
       "0  comments  Number of comments. Target variable   str"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what_is(dictionary, 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[0]; #using this to reference structure and build for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TruncatedSVD(n_components = 3,\n",
    "                 n_iter = 5,\n",
    "                 random_state = 1994)\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting comments using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "#### Load in the the data of scraped results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the number of comments was low or high. Compute the median number of comments and create a new binary variable that is true when the number of comments is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the number of comments here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW number of comments.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extremely popular threads. We don't _have_ to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of comment numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low number of comments using Sklearn. Start by ONLY using the subreddit as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a thread title.\n",
    "- For example, create a feature that represents whether 'cat' is in the title or whether 'funny' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value?\n",
    "- After creating these variables, use count-vectorizer to create features based on the words in the thread titles.\n",
    "- Build a new random forest model with subreddit and these new features included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process with a non-tree-based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Use Count Vectorizer from scikit-learn to create features from the thread titles. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS\n",
    "Refer to the README for the bonus parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "4239e458-28bd-4675-8db3-c1d9c02b9854"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
