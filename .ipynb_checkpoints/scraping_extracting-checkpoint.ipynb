{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "# Reddit API project\n",
    "## Sonyah Seiden\n",
    "\n",
    "### Goals:\n",
    "- Collect data via an API request\n",
    "- Build a binary predictor\n",
    "- Present information to a business-focused audience (non-technical)\n",
    "\n",
    "### Question:\n",
    "#### _What characteristics of a post on Reddit contribute most to the overall interaction? Measured here in number of comments._\n",
    "#### _Characteristics must include_:\n",
    "- Title of thread\n",
    "- Subreddit\n",
    "- Length of time up on Reddit\n",
    "- Number of comments (yi)\n",
    "\n",
    "#### _Model developed must incorporate:_\n",
    "- Classification model\n",
    "- Natural Language Processing\n",
    "- Threshold determined by _median_  number of comments\n",
    "\n",
    "#### _**BONUS FEATURES**:_\n",
    "- Use GridSearch Ridge and Lasso for this model to determine best hyperparameters\n",
    "- Turn pitch into a blog post and host on a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictionary used as reference\n",
    "https://github.com/reddit-archive/reddit/wiki/JSON  \n",
    "(Secondary data dictionary created in cleaning_modeling notebook for selected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the first step in my scraping process I'm going to make 1 request to identify where the information I want is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.reddit.com/hot.json\"\n",
    "headers = {'User-agent': 'Bleep Bloop'}\n",
    "res = requests.get(url, headers=headers)\n",
    "res.status_code #looking for a 2-- code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'kind']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json = res.json()\n",
    "sorted(data_json.keys()) #checking what keys exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'approved_at_utc': None,\n",
       "  'approved_by': None,\n",
       "  'archived': False,\n",
       "  'author': 'capt_geo',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_template_id': None,\n",
       "  'author_flair_text': None,\n",
       "  'banned_at_utc': None,\n",
       "  'banned_by': None,\n",
       "  'can_gild': False,\n",
       "  'can_mod_post': False,\n",
       "  'clicked': False,\n",
       "  'contest_mode': False,\n",
       "  'created': 1527720026.0,\n",
       "  'created_utc': 1527691226.0,\n",
       "  'distinguished': None,\n",
       "  'domain': 'usatoday.com',\n",
       "  'downs': 0,\n",
       "  'edited': False,\n",
       "  'gilded': 1,\n",
       "  'hidden': False,\n",
       "  'hide_score': True,\n",
       "  'id': '8n93wl',\n",
       "  'is_crosspostable': False,\n",
       "  'is_reddit_media_domain': False,\n",
       "  'is_self': False,\n",
       "  'is_video': False,\n",
       "  'likes': None,\n",
       "  'link_flair_css_class': None,\n",
       "  'link_flair_text': None,\n",
       "  'locked': False,\n",
       "  'media': None,\n",
       "  'media_embed': {},\n",
       "  'media_only': False,\n",
       "  'mod_note': None,\n",
       "  'mod_reason_by': None,\n",
       "  'mod_reason_title': None,\n",
       "  'mod_reports': [],\n",
       "  'name': 't3_8n93wl',\n",
       "  'no_follow': False,\n",
       "  'num_comments': 1995,\n",
       "  'num_crossposts': 2,\n",
       "  'num_reports': None,\n",
       "  'over_18': False,\n",
       "  'parent_whitelist_status': 'all_ads',\n",
       "  'permalink': '/r/news/comments/8n93wl/ambienmaker_to_roseanne_racism_is_not_a_side/',\n",
       "  'pinned': False,\n",
       "  'post_categories': None,\n",
       "  'post_hint': 'link',\n",
       "  'preview': {'enabled': False,\n",
       "   'images': [{'id': 'lnDij1ooNqRYohhtRU8fD4o2XkIrZQljWXqZC753s2o',\n",
       "     'resolutions': [{'height': 56,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=dd186d161f95b6ece9209ed80a3ed1de',\n",
       "       'width': 108},\n",
       "      {'height': 113,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=c737ee3252cbb8e81d931b9ebdd09965',\n",
       "       'width': 216},\n",
       "      {'height': 168,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=335ded609f2dea8a0e072291c60baa71',\n",
       "       'width': 320},\n",
       "      {'height': 336,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=fa00f3f9eccaa0c7d84157984f9cb9b1',\n",
       "       'width': 640},\n",
       "      {'height': 504,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=7ed890ae452e3627c83c21f8dec5ce63',\n",
       "       'width': 960},\n",
       "      {'height': 567,\n",
       "       'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=9f01bff2b1539e15ca8a71d7a08e09b0',\n",
       "       'width': 1080}],\n",
       "     'source': {'height': 1680,\n",
       "      'url': 'https://i.redditmedia.com/2Fvhnc3nQXDgCGCBgU-ECWiA90U_zZWsRdwL50MORB8.jpg?s=f9ccad3b6934a875c5244ad4af3912e0',\n",
       "      'width': 3200},\n",
       "     'variants': {}}]},\n",
       "  'pwls': 6,\n",
       "  'quarantine': False,\n",
       "  'removal_reason': None,\n",
       "  'report_reasons': None,\n",
       "  'saved': False,\n",
       "  'score': 20818,\n",
       "  'secure_media': None,\n",
       "  'secure_media_embed': {},\n",
       "  'selftext': '',\n",
       "  'selftext_html': None,\n",
       "  'send_replies': False,\n",
       "  'spoiler': False,\n",
       "  'stickied': False,\n",
       "  'subreddit': 'news',\n",
       "  'subreddit_id': 't5_2qh3l',\n",
       "  'subreddit_name_prefixed': 'r/news',\n",
       "  'subreddit_subscribers': 16076197,\n",
       "  'subreddit_type': 'public',\n",
       "  'suggested_sort': None,\n",
       "  'thumbnail': 'default',\n",
       "  'thumbnail_height': 73,\n",
       "  'thumbnail_width': 140,\n",
       "  'title': 'Ambien-maker to Roseanne: Racism is not a side effect of our drug',\n",
       "  'ups': 20818,\n",
       "  'url': 'https://www.usatoday.com/story/news/nation-now/2018/05/30/roseanne-barr-blames-ambien-zolpidem-drug-real-side-effects/654683002/',\n",
       "  'user_reports': [],\n",
       "  'view_count': None,\n",
       "  'visited': False,\n",
       "  'whitelist_status': 'all_ads',\n",
       "  'wls': 6},\n",
       " 'kind': 't3'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_json['data']['children'][0])\n",
    "#searching within to find exactly where the information I want is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_8n7npw'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['after'] #this shows the name of the last post in my list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have tested my scrape, and will use this structure to build out a for loop and generate thousands of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'Bleep Bloop'}\n",
    "#Creating a custom agent for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'after': 't3_8n7npw'} #pulled the name from above\n",
    "requests.get(url, params=params, headers=headers)\n",
    "#displaying how to adapt the for loop using params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0\n",
      "iteration1\n",
      "iteration2\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "after = 't3_8n38jx'\n",
    "#t3_8n1ha7'\n",
    "#t3_8n0e3h'\n",
    "#t3_8n14rc'\n",
    "for i in range(3):\n",
    "    #Worked my way up to 500 posts with 3 seconds (used that range when I actually scraped)\n",
    "    #This allowed me to pull more unique posts and once, and avoid too many doubles\n",
    "    #Tested how long it took/checked for rejections by adjusting time.sleep(i)\n",
    "    if i % 1 == 0:\n",
    "        print('iteration{}'.format(i))\n",
    "    if after == None:\n",
    "        params = {}\n",
    "    else:\n",
    "        params = {'after': after}\n",
    "    url = 'https://www.reddit.com/hot.json'\n",
    "    res = requests.get(url, params=params, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        posts.extend([c['data'] for c in  the_json['data']['children']])\n",
    "        #including a list comprehension allows me to extract data in an organized way\n",
    "        #this means less cleaning later on!\n",
    "        pd.DataFrame(posts).to_csv('display.csv', index=False)\n",
    "        #saving to display.csv for a reference to see how this works.\n",
    "        #Won't be using this dataset because it will be doubles of posts I already have\n",
    "        #display.csv is left in the repository for your reference\n",
    "        after = the_json['data']['after']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "    time.sleep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts) #checking the number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([p['name'] for p in posts])) #checking how many are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_8n806v'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[74]['name'] #pull the name of the last post to run another loop & avoid unecessary doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I've run the loop 4 times and scraped a few thousands unique posts. I saved them all to csv's and am now going to import then and combine them before extracting and cleaning.   \n",
    "### I am doing this step by step to ensure I do not mess it up, and to keep track of how the size grows with each additional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 85)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('my_precious.csv');\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5475, 85)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1 = pd.read_csv('my_precious_1.csv');\n",
    "dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7493, 85)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2 = pd.read_csv('my_precious_2.csv');\n",
    "dataset_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10655, 85)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_3 = pd.read_csv('my_precious_3.csv');\n",
    "dataset_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10655, 85)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_4 = pd.read_csv('my_precious_3.csv')\n",
    "#same csv number because I overwrote it, but posts were saved before this happened\n",
    "#they exist within unique_posts\n",
    "dataset_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12975, 85)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = dataset.append(dataset_1);\n",
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20468, 85)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = new_dataset.append(dataset_2);\n",
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31123, 85)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = new_dataset.append(dataset_3);\n",
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posts = new_dataset.drop_duplicates(subset='permalink')\n",
    "#Permalink is the only unique component of each post.\n",
    "#This ensures we eliminate all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11047"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_posts.to_csv('unique_posts.csv', index=False)\n",
    "#Saving this to a CSV because I learned that mistake early on.\n",
    "#Commented it out now to avoid overwriting it and losing any posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After I initially did this and tested the csv in my other notebook, I decided to add a few more unique posts. I ran the for loop and saved it as dataset_4, then reloaded in the original unique_posts and appended it directly to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dataset_4 = dataset_4.drop_duplicates(subset='permalink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posts = pd.read_csv('./unique_posts.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posts = unique_posts.append(unique_dataset_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18018, 85)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posts.to_csv('unique_posts.csv', index=False)\n",
    "#resaving full dataset of unique_posts with new set of posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Now that I saved all unique posts and have a sufficient amount, I'm moving over to my cleaningmodeling notebook to keep things organized._"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
